{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP - HMM et Viterbi DEBONNE & DI CARLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Création d'un détecteur d'entités nommées en utilisant un HMM. Les états représenteront les étiquettes des entités, les observations seront les mots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 - Statistiques sur le corpus d'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "import math\n",
    "\n",
    "fichier = open(\"train.ester1.cut.bio\", \"r\")\n",
    "# Liste des phrases\n",
    "train = fichier.read().split(\"\\n\\n\")\n",
    "fichier.close()\n",
    "\n",
    "# Liste de tous les couples\n",
    "couples = []\n",
    "mots = []\n",
    "etiquettes = []\n",
    "\n",
    "for phrase in train:\n",
    "    temp = phrase.split(\"\\n\")\n",
    "    for t in temp:\n",
    "        if t != '':\n",
    "            couples.append(t)\n",
    "            t1 = t.split(\" \")\n",
    "            mots.append(t1[0])\n",
    "            etiquettes.append(t1[1])\n",
    "\n",
    "etiquettes = list(set(etiquettes))\n",
    "mots = list(set(mots))\n",
    "print(\"Nombre de mots differents: \" + str(len(mots)))\n",
    "print(\"Nombre d etiquettes differents: \" + str(len(etiquettes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des phrases split en couple de mot\n",
    "def splitPhrases( desPhrases ):\n",
    "    phrasesSplit = []\n",
    "\n",
    "    for phrase in desPhrases:\n",
    "        temp = []\n",
    "        for couple in phrase.split(\"\\n\"):\n",
    "            temp.append(couple.split(\" \"))\n",
    "        phrasesSplit.append(temp)\n",
    "        \n",
    "    return phrasesSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 - Apprentissage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Nous avons 3 paramètres à calculer:\n",
    "- les probabilités initiales pi\n",
    "- la matrice de transition A\n",
    "- les probabilités d'émission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On travail sur le corpus d'entrainement\n",
    "trainSplit = splitPhrases( train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilités initiales pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des premieres etiquettes de chaque phrase\n",
    "listfirstetiq = []\n",
    "listeprobainit = []\n",
    "    \n",
    "for p in trainSplit:\n",
    "    # Premiere etiquette de chaque phrase\n",
    "    listfirstetiq.append(p[0][1])\n",
    "\n",
    "nbphrase = len(listfirstetiq) # Nombre total de phrases du corpus\n",
    "\n",
    "# Calcul des logs probabilites pour chaque etiquette\n",
    "for e in etiquettes:\n",
    "    # On rajoute 1 occurence pour chaque étiquette pour gérer les logs(0)\n",
    "    p = (1.0+(listfirstetiq.count(e)))/(nbphrase+len(etiquettes))\n",
    "    p = math.log10(p) \n",
    "    listeprobainit.append(p)\n",
    "\n",
    "# Memorisation dans un dictionnaire\n",
    "pi = dict(zip(etiquettes,listeprobainit))\n",
    "\n",
    "# Affichage\n",
    "print(\"Log probabiltés initiales : \")\n",
    "for e,p in pi.items():\n",
    "    print(\"\\t\", str(e) + \": \"+ str(round(p,3)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de transition A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpt = 0\n",
    "tabDic = []\n",
    "# Création d'un dictionnaire par étiquettes\n",
    "for eti in etiquettes:\n",
    "    \n",
    "    probas = [0] * len(etiquettes)\n",
    "    dic = dict(zip(etiquettes,probas))\n",
    "\n",
    "    for sentence in trainSplit:\n",
    "        cpt += 1\n",
    "        # On vérifie que le dernier couple ne soit pas vide\n",
    "        if sentence[(len(sentence)-1)] == ['']:\n",
    "            end = (len(sentence)-2)\n",
    "        else:\n",
    "            end = (len(sentence)-1)\n",
    "\n",
    "        # Calcul sur toutes les phrases du nombre de transition d'un état à l'autre\n",
    "        for i in range(0, end):\n",
    "            if sentence[i][1] == eti:\n",
    "                dic[sentence[i+1][1]] += 1\n",
    "\n",
    "    # Ajout +1 partout pour éviter les logs(0)\n",
    "    for d in dic:\n",
    "        dic[d] += 1\n",
    "        \n",
    "    # Total -> Probabilités\n",
    "    tot = sum(dic.values())\n",
    "    for et in dic:\n",
    "        dic[et] = math.log10(dic[et]/(tot+15))\n",
    "\n",
    "    tabDic.append(dic)\n",
    "\n",
    "# Création du dictionnaire contenant comme clé l'étiquette et comme valeur son dictionnaire de transition\n",
    "A = dict(zip(etiquettes,tabDic))\n",
    "for t in A:\n",
    "    print(t)\n",
    "    for k,v in A[t].items():\n",
    "        print(\"\\t\", k, \": \", round(v,3))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilités d'émission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabDic = []\n",
    "# Création d'un dictionnaire par étiquettes\n",
    "for eti in etiquettes:\n",
    "\n",
    "    probas = [0] * len(mots)\n",
    "    dic = dict(zip(mots,probas))\n",
    "    \n",
    "    for sentence in trainSplit:\n",
    "        \n",
    "        for couple in sentence:\n",
    "            if couple != ['']:\n",
    "                if couple[1] == eti:\n",
    "                    dic[couple[0]] += 1\n",
    "    \n",
    "    # Ajout +1 partout pour éviter les logs(0)\n",
    "    for d in dic:\n",
    "        dic[d] += 1\n",
    "        \n",
    "    # Total -> Probabilités\n",
    "    tot = sum(dic.values())\n",
    "    for et in dic:\n",
    "        dic[et] = math.log10(dic[et]/(tot+15))\n",
    "        \n",
    "    tabDic.append(dic)    \n",
    "\n",
    "emission = dict(zip(etiquettes,tabDic))\n",
    "\"\"\"\n",
    "for t in emission:\n",
    "    print(t)\n",
    "    for k,v in emission[t].items():\n",
    "        print(\"\\t\", k, \": \", round(v,3))\n",
    "    print(\"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3 - Algorithme de Viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On travail sur le corpus de développement\n",
    "fichier = open(\"dev.ester1.cut.bio\", \"r\")\n",
    "dev = fichier.read().split(\"\\n\\n\")\n",
    "devSplit = splitPhrases( dev )\n",
    "fichier.close()\n",
    "\n",
    "print(devSplit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erreurSequence( seq1, seq2 ):\n",
    "    if(len(seq1)!=len(seq2)):\n",
    "        return -1\n",
    "    else:\n",
    "        nbcorrect=0\n",
    "        for i in range(0,len(seq1)):\n",
    "            if(seq1[i]==seq2[i]):\n",
    "                nbcorrect+=1\n",
    "        return (1-(nbcorrect/len(seq1))) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cheminPhrase( unePhrase ):\n",
    "    chemin = []\n",
    "    \n",
    "    # Séparaion Etiquettes / Observations\n",
    "    tabObservations = []\n",
    "    tabEtiquettes = []\n",
    "\n",
    "    for mot in unePhrase:\n",
    "        tabObservations.append( mot[0] )\n",
    "        tabEtiquettes.append( mot[1] )\n",
    "\n",
    "    # Pour la séquence d'observation on génère la séquence d'étiquette ayant la probabilité maximale\n",
    "    tabViterbiProba = []\n",
    "    tabViterbiEti = []\n",
    "    lastDic = {}\n",
    "    for obs in tabObservations:\n",
    "        dicProba = {}\n",
    "        dicEti = {}\n",
    "\n",
    "        # Si c'est le premier mot de la phrase les calculs ne sont pas les mêmes -> utilisation de pi\n",
    "        if tabObservations.index(obs) == 0:\n",
    "            for et in etiquettes:\n",
    "                dicProba[et] = pi[et] + emission[et][obs]\n",
    "                dicEti[et] = \"-\"            \n",
    "\n",
    "        else:\n",
    "            for et in etiquettes:\n",
    "                probTemp = {}\n",
    "                for eti in etiquettes:\n",
    "                    probTemp[eti] = ( lastDic[eti] + A[eti][et] + emission[et][obs] )\n",
    "\n",
    "                dicProba[et] = max(zip(probTemp.values(), probTemp.keys()))[0]\n",
    "                dicEti[et] = max(zip(probTemp.values(), probTemp.keys()))[1]\n",
    "\n",
    "        tabViterbiProba.append(dicProba)\n",
    "        tabViterbiEti.append(dicEti)\n",
    "        lastDic = dicProba\n",
    "\n",
    "    \"\"\"# Affichage\n",
    "    mot=0\n",
    "    for tab in tabViterbiProba:\n",
    "        print(tabObservations[mot])\n",
    "        mot+=1\n",
    "\n",
    "        for e,p in tab.items():\n",
    "            print(\"\\t\", str(e) + \": \"+ str(round(p,3)))\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "    mot=0\n",
    "    for tab in tabViterbiEti:\n",
    "        print(tabObservations[mot])\n",
    "        mot+=1\n",
    "\n",
    "        for e,p in tab.items():\n",
    "            print(\"\\t\", str(e) + \": \"+ p)\n",
    "\n",
    "        print(\"\\n\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # Chainage arrière pour connaitre le chemin de probabilité maximale   \n",
    "    i = len(tabViterbiProba) - 1\n",
    "    eti = \"\"\n",
    "    while i >= 0:\n",
    "        if i == len(tabViterbiProba) - 1:\n",
    "            dic = tabViterbiProba[i]\n",
    "            eti = max(zip(dic.values(), dic.keys()))[1]\n",
    "            chemin.append(eti)\n",
    "        else:\n",
    "            chemin.append(tabViterbiEti[i+1][eti])\n",
    "            \n",
    "        i -= 1\n",
    "             \n",
    "    chemin = chemin[::-1]\n",
    "    \n",
    "    print( \"\\nChemin prédis:\\t\", chemin, \"\\nChemin réel:\\t\", tabEtiquettes )\n",
    "    \n",
    "    \n",
    "    return round(erreurSequence( chemin, tabEtiquettes ), 2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase in devSplit:\n",
    "    print( cheminPhrase( phrase ), \"% d'erreur\" )\n",
    "    \n",
    "# Nous avons une erreur pour la phrase 8 car le mot \"ramenés\" est présent dans le corpus de développement\n",
    "# mais pas dans le corpus train. La probabilité d'émission concernant ce mot n'est donc pas connue.\n",
    "# Nous ne savons donc pas comment pallier à ce problème..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
